{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RFMS_Score', 'TransactionCount', 'ProductCategory_woe',\n",
      "       'ChannelId_woe', 'AverageTransactionAmount', 'TransactionAmountStd',\n",
      "       'RiskLabel', 'Value', 'Amount', 'ProviderId_woe',\n",
      "       'TotalTransactionAmount', 'Frequency_woe', 'Monetary_woe',\n",
      "       'Recency_woe', 'Seniority_woe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset (replace 'path_to_your_data.csv' with your actual file path)\n",
    "df = pd.read_csv('C:/Users/elbet/OneDrive/Desktop/Ten/week-6/github-notebook/Credit-Scoring-Model-/data/data/df_woe.csv')\n",
    "df = df.drop(columns=['TransactionId','SubscriptionId','AccountId','ProductId','BatchId',\n",
    "                      'CustomerId','CurrencyCode','CountryCode','TransactionStartTime',\n",
    "                      'FraudResult','TransactionYear','TransactionDay','PricingStrategy','TransactionHour','TransactionMonth'])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['RiskLabel'] = label_encoder.fit_transform(df['RiskLabel'])\n",
    "\n",
    "# Split data into features (X) and target variable (y)\n",
    "X = df.drop(columns=['RiskLabel'])  # Features\n",
    "y = df['RiskLabel']  # Target variable\n",
    "\n",
    "# Split the data: 80% training, 10% validation, 10% testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42, stratify=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    return roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n",
      "Epoch 0, Best Random Forest Validation Accuracy: 1.0, Best Params: {'bootstrap': True, 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 150}\n",
      "\n",
      "Epoch 2/3\n",
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n",
      "Epoch 1, Best Random Forest Validation Accuracy: 1.0, Best Params: {'bootstrap': True, 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 150}\n",
      "\n",
      "Epoch 3/3\n",
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n",
      "Epoch 2, Best Random Forest Validation Accuracy: 1.0, Best Params: {'bootstrap': True, 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 150}\n",
      "\n",
      "Best Random Forest Validation Score Achieved: 1.0\n",
      "\n",
      "Best Random Forest Model Performance on Test Set:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "ROC-AUC: 1.0000\n",
      "Best Random Forest model saved as 'best_rf_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Random Forest hyperparameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [150, 300, 600],\n",
    "    'max_depth': [15, 25, 35, None],\n",
    "    'min_samples_split': [3, 6, 12],\n",
    "    'min_samples_leaf': [2, 3, 5],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "# Early stopping configuration\n",
    "best_model = None\n",
    "best_val_score = 0\n",
    "patience = 3  # Number of epochs with no improvement to wait before stopping\n",
    "stopping_counter = 0\n",
    "\n",
    "# Training loop with early stopping for RandomForestClassifier with hyperparameter tuning\n",
    "for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "    print(f\"\\nEpoch {epoch + 1}/{3}\")\n",
    "\n",
    "    # Hyperparameter tuning using GridSearchCV for Random Forest\n",
    "    rf_grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                                  param_grid=rf_param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "    rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model and evaluate on the validation set\n",
    "    best_rf_model = rf_grid_search.best_estimator_\n",
    "    val_preds = best_rf_model.predict(X_val)\n",
    "    val_score = accuracy_score(y_val, val_preds)  # Use appropriate metric\n",
    "\n",
    "    print(f'Epoch {epoch}, Best Random Forest Validation Accuracy: {val_score}, Best Params: {rf_grid_search.best_params_}')\n",
    "\n",
    "    # Check if the current model is the best model\n",
    "    if val_score > best_val_score:\n",
    "        best_val_score = val_score\n",
    "        best_model = best_rf_model\n",
    "        stopping_counter = 0  # Reset counter if there is improvement\n",
    "    else:\n",
    "        stopping_counter += 1\n",
    "\n",
    "    # Early stopping condition\n",
    "    if stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered for Random Forest!\")\n",
    "        break\n",
    "\n",
    "# After early stopping, the best Random Forest model is used\n",
    "print(\"\\nBest Random Forest Validation Score Achieved:\", best_val_score)\n",
    "\n",
    "# Evaluate the best Random Forest model on the test set\n",
    "print(\"\\nBest Random Forest Model Performance on Test Set:\")\n",
    "evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "# Save the best Random Forest model to a .pkl file\n",
    "with open('C:/Users/elbet/OneDrive/Desktop/Ten/week-6/github-notebook/Credit-Scoring-Model-/rf_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "print(\"Best Random Forest model saved as 'best_rf_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Epoch 0, Best GBM Validation Accuracy: 1.0, Best Params: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_depth': 3, 'learning_rate': 0.01}\n",
      "\n",
      "Epoch 2/3\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Epoch 1, Best GBM Validation Accuracy: 1.0, Best Params: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_depth': 3, 'learning_rate': 0.01}\n",
      "\n",
      "Epoch 3/3\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Epoch 2, Best GBM Validation Accuracy: 1.0, Best Params: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_depth': 3, 'learning_rate': 0.01}\n",
      "\n",
      "Best GBM Validation Score Achieved: 1.0\n",
      "\n",
      "Best GBM Model Performance on Test Set:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "ROC-AUC: 1.0000\n",
      "Best Gradient Boosting model saved as 'best_gbm_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting hyperparameter grid\n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Early stopping for Gradient Boosting with hyperparameter tuning\n",
    "best_gbm_model = None\n",
    "best_val_score_gbm = 0\n",
    "stopping_counter_gbm = 0\n",
    "\n",
    "for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "    print(f\"\\nEpoch {epoch + 1}/{3}\")\n",
    "\n",
    "    # Hyperparameter tuning using RandomizedSearchCV for Gradient Boosting\n",
    "    gbm_random_search = RandomizedSearchCV(estimator=GradientBoostingClassifier(random_state=42),\n",
    "                                           param_distributions=gbm_param_grid, n_iter=50, cv=5, random_state=42, n_jobs=-1, verbose=1)\n",
    "    gbm_random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model and evaluate on the validation set\n",
    "    best_gbm_model = gbm_random_search.best_estimator_\n",
    "    val_preds_gbm = best_gbm_model.predict(X_val)\n",
    "    val_score_gbm = accuracy_score(y_val, val_preds_gbm)\n",
    "\n",
    "    print(f'Epoch {epoch}, Best GBM Validation Accuracy: {val_score_gbm}, Best Params: {gbm_random_search.best_params_}')\n",
    "\n",
    "    if val_score_gbm > best_val_score_gbm:\n",
    "        best_val_score_gbm = val_score_gbm\n",
    "        best_gbm_model = best_gbm_model\n",
    "        stopping_counter_gbm = 0\n",
    "    else:\n",
    "        stopping_counter_gbm += 1\n",
    "\n",
    "    if stopping_counter_gbm >= patience:\n",
    "        print(\"Early stopping triggered for GBM!\")\n",
    "        break\n",
    "\n",
    "# After early stopping for Gradient Boosting, the best GBM model is used\n",
    "print(\"\\nBest GBM Validation Score Achieved:\", best_val_score_gbm)\n",
    "\n",
    "# Evaluate the best Gradient Boosting model on the test set\n",
    "print(\"\\nBest GBM Model Performance on Test Set:\")\n",
    "evaluate_model(best_gbm_model, X_test, y_test)\n",
    "\n",
    "# Save the best GBM model to a .pkl file\n",
    "with open('C:/Users/elbet/OneDrive/Desktop/Ten/week-6/github-notebook/Credit-Scoring-Model-/gbm_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_gbm_model, file)\n",
    "\n",
    "print(\"Best Gradient Boosting model saved as 'best_gbm_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier that combines the best Random Forest and Gradient Boosting models\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', best_model),\n",
    "    ('gbm', best_gbm_model)\n",
    "], voting='soft')\n",
    "\n",
    "# Train the Voting Classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Voting Classifier\n",
    "print(\"\\nVoting Classifier Performance:\")\n",
    "evaluate_model(voting_clf, X_test, y_test)\n",
    "\n",
    "# Save the Voting Classifier to a .pkl file\n",
    "with open('C:/Users/elbet/OneDrive/Desktop/Ten/week-6/github-notebook/Credit-Scoring-Model-/final_model.pkl', 'wb') as file:\n",
    "    pickle.dump(voting_clf, file)\n",
    "\n",
    "print(\"Voting Classifier model saved as 'ensemble_voting_model.pkl'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jojo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
